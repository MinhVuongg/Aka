import peft
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import os
from src.config.config import MODEL_NAME
from src.train.lora_trainer import LoRATrainer
import transformers
transformers.utils.logging.enable_explicit_format()
transformers.trainer.USE_TF = False 

class LoRATrainer_CodeT5P_2B(LoRATrainer):
    def __init__(self, model_name):
        """Kh·ªüi t·∫°o m√¥ h√¨nh v·ªõi LoRA"""
        super().__init__(model_name)
        self.add_lora()  # Th√™m LoRA v√†o m√¥ h√¨nh

    @staticmethod
    def load_model(model_name):
        """T·∫£i m√¥ h√¨nh CodeT5p v√† tokenizer."""
        try:
            model = AutoModelForSeq2SeqLM.from_pretrained(
                model_name,
                device_map="auto",  # T·ª± ƒë·ªông nh·∫≠n di·ªán thi·∫øt b·ªã
                trust_remote_code=True,  # B·∫Øt bu·ªôc cho CodeT5p
                torch_dtype=torch.float16
            )
            tokenizer = AutoTokenizer.from_pretrained(model_name, safe_serialization=True)
            print(f" ƒê√£ t·∫£i m√¥ h√¨nh {model_name} th√†nh c√¥ng.")
            return model, tokenizer
        except Exception as e:
            print(f" L·ªói khi t·∫£i m√¥ h√¨nh: {e}")
            raise

    def add_lora(self, r=16, lora_alpha=32, lora_dropout=0.1):
        """Th√™m LoRA v√†o CodeT5p-2B."""
        target_modules = self.get_lora_target_modules()

        lora_config = peft.LoraConfig(
            r=r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",  
            task_type="SEQ_2_SEQ_LM"
        )

        self.model = peft.get_peft_model(self.model, lora_config)
        print(" LoRA ƒë√£ ƒë∆∞·ª£c th√™m v√†o m√¥ h√¨nh CodeT5p.")

        self.print_trainable_parameters()

    def get_lora_target_modules(self):
        """X√°c ƒë·ªãnh c√°c module c·∫ßn √°p d·ª•ng LoRA."""
        target_modules = []

        # Encoder layers
        num_encoder_layers = 20  
        for i in range(num_encoder_layers):
            target_modules.extend([
                f"encoder.h.{i}.attn.qkv_proj",
                f"encoder.h.{i}.attn.out_proj"
            ])

        # Decoder layers (Self-Attention)
        num_decoder_layers = 32  
        for i in range(num_decoder_layers):
            target_modules.extend([
                f"decoder.transformer.h.{i}.attn.qkv_proj",
                f"decoder.transformer.h.{i}.attn.out_proj"
            ])

        # Cross-Attention (Ch·ªâ c√≥ ·ªü l·ªõp 31, c·∫ßn x√°c nh·∫≠n)
        target_modules.extend([
            f"decoder.transformer.h.31.crossattention.qkv_proj",
            f"decoder.transformer.h.31.crossattention.out_proj"
        ])

        print(f" Target Modules: {target_modules}")
        return target_modules

    def print_trainable_parameters(self):
        """In s·ªë l∆∞·ª£ng tham s·ªë c√≥ th·ªÉ hu·∫•n luy·ªán."""
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        all_params = sum(p.numel() for p in self.model.parameters())

        print(f"üîπ Trainable params: {trainable_params} || Total params: {all_params} || Ratio: {100 * trainable_params / all_params:.2f}%")

    def train(self):
        """Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi LoRA"""
        # G·ªçi ph∆∞∆°ng th·ª©c hu·∫•n luy·ªán t·ª´ Trainer
        super().train()

        # ƒê∆∞·ªùng d·∫´n l∆∞u m√¥ h√¨nh sau khi train
        save_path = "/workspace/aka-llm/aka-output/2025-03-11-18-39/model_checkpoint/"
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(save_path, exist_ok=True)

        # L∆∞u m√¥ h√¨nh v√† tokenizer
        self.model.save_pretrained(save_path, safe_serialization=True)
        self.model.config.decoder_start_token_id = self.tokenizer.cls_token_id 
        self.tokenizer.save_pretrained(save_path)

        print(f"M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {save_path}")
